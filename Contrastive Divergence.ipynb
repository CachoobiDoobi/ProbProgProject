{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027c4c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer import MCMC, NUTS\n",
    "from pyro import poutine\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.util import (\n",
    "    torch_item,\n",
    ")\n",
    "from pyro.util import check_if_enumerated, warn_if_nan\n",
    "from pyro.infer.autoguide.guides import AutoGuideList\n",
    "import pyro.poutine as poutine\n",
    "from pyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec29704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # define the hyperparameters that control the Beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the Beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "488dcfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide_zero(data):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0.\n",
    "    # - because we invoke constraints.positive, the optimizer\n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    return pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "    \n",
    "def guide_t(sample):\n",
    "    # i think we have a problem here -> we get a fairness of 0.5 and it should actually be skewed towards heads based on the data\n",
    "    nuts_kernel = NUTS(model)\n",
    "    mcmc = MCMC(\n",
    "        nuts_kernel,\n",
    "        num_samples=10,\n",
    "        warmup_steps=10,\n",
    "        num_chains=1,\n",
    "    )\n",
    "    mcmc.run(sample)\n",
    "    samples = mcmc.get_samples()\n",
    "    return samples\n",
    "#     return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71328206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should replace trace_elbo\n",
    "# def simple_elbo(model, guide_t, guide_zero,  *args, **kwargs):\n",
    "#     # run the guide and trace its execution\n",
    "#     guide_0_trace = poutine.trace(guide_zero).get_trace( *args, **kwargs)\n",
    "#     guide_t_trace = poutine.trace(guide_t).get_trace( *args, **kwargs)\n",
    "#     # run the model and replay it against the samples from the guide\n",
    "#     model_trace = poutine.trace(\n",
    "#         poutine.replay(model, trace=guide_t_trace)).get_trace( *args, **kwargs)\n",
    "    \n",
    "#     # osntrative loss\n",
    "#     l_diff= (model_trace.log_prob_sum() - guide_0_trace.log_prob_sum()) - (model_trace.log_prob_sum() - guide_t_trace.log_prob_sum())\n",
    "#     l_vcd = l_diff +  guide_0_trace.log_prob_sum() - guide_t_trace.log_prob_sum()\n",
    "    \n",
    "#     return -l_vcd\n",
    "def simple_elbo(model, guide_t, guide_zero,  *args, **kwargs):\n",
    "    # run the guide and trace its execution\n",
    "    guide_0_trace = poutine.trace(guide_zero).get_trace( *args, **kwargs)\n",
    "    guide_t_trace = poutine.trace(guide_t).get_trace( *args, **kwargs)\n",
    "    # run the model and replay it against the samples from the guide\n",
    "    model_trace = poutine.trace(\n",
    "        poutine.replay(model, trace=guide_t_trace)).get_trace( *args, **kwargs)\n",
    "    \n",
    "    # osntrative loss\n",
    "    l_diff= (model_trace.log_prob_sum() - guide_0_trace.log_prob_sum()) - (model_trace.log_prob_sum() - guide_t_trace.log_prob_sum())\n",
    "    l_vcd = l_diff +  guide_0_trace.log_prob_sum() - guide_t_trace.log_prob_sum()\n",
    "    \n",
    "    return -l_vcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da341f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "n_steps = 2 if smoke_test else 200\n",
    "\n",
    "assert pyro.__version__.startswith('1.8.6')\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93e49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data with 6 observed heads and 4 observed tails\n",
    "data = []\n",
    "for _ in range(90):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(10):\n",
    "    data.append(torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12b447f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoCallable()\n"
     ]
    }
   ],
   "source": [
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "\n",
    "guide = AutoGuideList(model)\n",
    "guide.append(guide_zero)\n",
    "guide.append(guide_t)\n",
    "print(guide[0])\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=simple_elbo)\n",
    "# svi = SVI(model, guide_zero, optimizer, loss=Trace_ELBO())\n",
    "guide[0]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ba659b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\pyro\\infer\\svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[1;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\pyro\\infer\\svi.py:83\u001b[0m, in \u001b[0;36mSVI.__init__.<locals>._loss_and_grads\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_loss_and_grads\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 83\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(loss_val, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     85\u001b[0m         loss_val\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36msimple_elbo\u001b[1;34m(model, guide_t, guide_zero, *args, **kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_elbo\u001b[39m(model, guide_t, guide_zero,  \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# run the guide and trace its execution\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     guide_0_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguide_zero\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     guide_t_trace \u001b[38;5;241m=\u001b[39m poutine\u001b[38;5;241m.\u001b[39mtrace(guide_t)\u001b[38;5;241m.\u001b[39mget_trace( \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# run the model and replay it against the samples from the guide\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\pyro\\poutine\\trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\pyro\\poutine\\trace_messenger.py:174\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    172\u001b[0m )\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    176\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\pyro\\nn\\module.py:449\u001b[0m, in \u001b[0;36mPyroModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyro_context:\n\u001b[1;32m--> 449\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    451\u001b[0m         pyro\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_poutine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyro_context\u001b[38;5;241m.\u001b[39mactive\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m _is_module_local_param_enabled()\n\u001b[0;32m    454\u001b[0m     ):\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_module_local_param_usage()\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\pyro\\infer\\autoguide\\guides.py:249\u001b[0m, in \u001b[0;36mAutoGuideList.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\Python\\Lib\\site-packages\\torch\\_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m    932\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    939\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pyro.clear_param_store()\n",
    "# These should be reset each training loop.\n",
    "\n",
    "losses = []\n",
    "for step in range(n_steps):  # Consider running for more steps.\n",
    "    loss = svi.step(guide, data)\n",
    "    losses.append(loss)\n",
    "    if step % 100 == 0:\n",
    "        logging.info(\"Elbo loss: {}\".format(loss))\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"SVI step\")\n",
    "plt.ylabel(\"ELBO loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "# here we use some facts about the Beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nBased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb97dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(model)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=1000)\n",
    "mcmc.run(data)\n",
    "samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['latent_fairness'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d05d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
